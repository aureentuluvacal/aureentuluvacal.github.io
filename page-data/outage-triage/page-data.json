{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/outage-triage/",
    "result": {"data":{"site":{"siteMetadata":{"title":"Cal Perez"}},"markdownRemark":{"id":"aeba1bd4-4e1d-535b-8de5-bcb183f9f9b3","excerpt":"We haven‚Äôt had an outage at Iris in like 8 months. I guess it was just a matter of time before it happened again, but I thought I‚Äôd have the necessary foresight‚Ä¶","timeToRead":3,"html":"<p>We haven‚Äôt had an outage at Iris in like 8 months. I guess it was just a matter of time before it happened again, but I thought I‚Äôd have the necessary foresight to not introduce code into master that exposes database flaws.</p>\n<p>Oh well. I learned what I have to do, so when it does happen again, it won‚Äôt feel so fan shit hitty.</p>\n<p>Now‚Ä¶database flaws, what does that mean? If you‚Äôve guessed an unindexed table, you‚Äôd be correct! üéâ</p>\n<p>Regrettably, I made a noob mistake merging a database migration adding an index to a beefy table and the code that required the migration to work effectively in the same release. I should have released the migration first then the feature after the table was done indexing in production. But it turns out, that wasn‚Äôt the direct cause of the outage.</p>\n<p>The migration just didn‚Äôt even run during the deployment in Kubernetes and it took me too long to realize that fact. I noticed the failed pod, looked at the logs, and realized the <code class=\"language-text\">rails db:migrate</code> process errored out on some faulty data.</p>\n<blockquote>\n<p>This is totally an aside, but if you ever see an error during a migration that adds a foreign key, check the table you‚Äôre trying to add the foreign key to for orphaned rows. Can‚Äôt add a key to something that doesn‚Äôt exist.</p>\n</blockquote>\n<p>After I fixed the data, the migrations ran. The index completed in 10 minutes and the app started working again because the database wasn‚Äôt overloaded with slow ass queries. In the aftermath I harnessed the leftover anger I had towards myself (fueled by the thought, ‚ÄúI‚Äôm the CTO, I should know better‚Äù) and cranked out a new outage contingency plan.</p>\n<h2>The new plan</h2>\n<p>Our plan is composed of three steps: (1) Diagnosis, (2) Reverting master, and (3) Make the fix.</p>\n<h3>Diagnosis</h3>\n<p>This step should take less than 20 minutes because it‚Äôs about identifying the fix, not actually fixing it. I laid out common sources of outages after a release, including the code itself, the code exposing a database flaw, and a deployment failure.</p>\n<p>We use Rollbar to track 4XX and 5XX errors, so finding the tracked issue that relates to a code issue is easy. The fix inherently could be more complicated because the faulty code could be anywhere.</p>\n<p>If there‚Äôs no obvious sign in Rollbar of a code issue, then the next thing to check is RDS in AWS for high CPU usage. When I looked at RDS during this last outage, CPU usage would spike intermittently. Running <code class=\"language-text\">SHOW PROCESSLIST;</code> within the instance proved that an index was missing because so many threads were stuck on the same query.</p>\n<p>The last source to check is the Kubernetes deployment. The logs for our <code class=\"language-text\">deploy-tasks</code> pod were pretty clear about the error that occurred during the migration.</p>\n<h3>Revert master</h3>\n<p>This step should have happened within the first 15-20 minutes of us noticing the outage. Instead, I focused my energy on finding the fix for a source I had yet to find because I looked at Kubernetes last and caused the app to be down for way longer than it should have been.</p>\n<p><img src=\"https://media.giphy.com/media/X7jENDat6V5Je/giphy.gif\" alt=\"image\"></p>\n<p>At least this is the easiest step.</p>\n<h3>Make the fix</h3>\n<p>The actual fix can vary in complexity depending on if it‚Äôs an actual code issue that‚Äôs the source of the outage. This makes sense since the codebase is so large; some areas you don‚Äôt want to touch and some of the newer areas are pleasant because <em>you</em> wrote them.</p>\n<p>Database and deployment issues are easier to fix, in my opinion, because the former requires a migration while the latter requires you to fix errors that occur during a single command (<code class=\"language-text\">rails db:migrate</code>).</p>\n<blockquote>\n<p>I covered how to add a new index asynchronously in <a href=\"https://caryssaperez.com/troubleshooting-db-cpu-error\">Troubleshooting Rails Database Connection Issue</a>.</p>\n</blockquote>\n<h2>Going forward</h2>\n<p>This process is definitely a living document so I can record different fixes for different causes of the outage. It‚Äôs also good to have for my two other engineers to reference in case I‚Äôm incapacitated or otherwise unavailable.</p>\n<p>Hopefully, I can minimize the noob mistakes.</p>","frontmatter":{"title":"How I caused another outage","date":"July 19, 2020","description":"And the new process I'll be using going forward.","subtitle":null,"tags":["Code"]}}},"pageContext":{"slug":"/outage-triage/","previous":{"fields":{"slug":"/logic-part-2/"},"frontmatter":{"title":"2 Logic 2 Furious","subtitle":null}},"next":{"fields":{"slug":"/warhammer-first-army/"},"frontmatter":{"title":"Finished my first, almost 500 point army!","subtitle":null}}}},
    "staticQueryHashes": ["3000541721","731186232"]}